{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('movie_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is one of those unfortunate films that su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay maybe it was because I happen to be in Ya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I love this movie, I can barely watch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man arrives in a strange, beautiful, sterile...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm sitting around going through movie listing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  This is one of those unfortunate films that su...          1\n",
       "1  Okay maybe it was because I happen to be in Ya...          1\n",
       "2  Although I love this movie, I can barely watch...          1\n",
       "3  A man arrives in a strange, beautiful, sterile...          1\n",
       "4  I'm sitting around going through movie listing...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       object\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isnull().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I rented this movie last week. I saw Kevin Spacey and Morgan Freeman were on it, so it seemed promising. And it was, until Justin Timberlake came on scene. He is a really bad actor and shouldn't be allowed to make a movie ever again. I mean, he is one of the most boring, uninspired actors I've ever seen. He puts absolutely no emotion to any of his lines whatsoever. Why the hell was he cast for the role of Josh Pollack? I think Matt Damon would have been a better choice.<br /><br />Kevin Spacey was another big disappointment. His character is so dull, it seems like a bad mix of his character in American Beauty and John Doe in Se7en. It might sound cool, but believe me, it's not.<br /><br />Now, Dylan McDermott's acting is very good. It's about one of the very few good things about this movie. He is just inspired.<br /><br />Morgan Freeman is good but nothing special. He has some really cool lines though.<br /><br />About the story, although it was a bit obvious and exaggerated at times it was good. I was expecting a big twist when Lazerov (Dylan McDermott) was killed, but nothing really happened.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc[1000,'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    cleanString = re.sub('\\W+',' ', text )\n",
    "    return  cleanString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented this movie last week I saw Kevin Spacey and Morgan Freeman were on it so it seemed promising And it was until Justin Timberlake came on scene He is a really bad actor and shouldn t be allowed to make a movie ever again I mean he is one of the most boring uninspired actors I ve ever seen He puts absolutely no emotion to any of his lines whatsoever Why the hell was he cast for the role of Josh Pollack I think Matt Damon would have been a better choice Kevin Spacey was another big disappointment His character is so dull it seems like a bad mix of his character in American Beauty and John Doe in Se7en It might sound cool but believe me it s not Now Dylan McDermott s acting is very good It s about one of the very few good things about this movie He is just inspired Morgan Freeman is good but nothing special He has some really cool lines though About the story although it was a bit obvious and exaggerated at times it was good I was expecting a big twist when Lazerov Dylan McDermott was killed but nothing really happened '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"I rented this movie last week. I saw Kevin Spacey and Morgan Freeman were on it, so it seemed promising. And it was, until Justin Timberlake came on scene. He is a really bad actor and shouldn't be allowed to make a movie ever again. I mean, he is one of the most boring, uninspired actors I've ever seen. He puts absolutely no emotion to any of his lines whatsoever. Why the hell was he cast for the role of Josh Pollack? I think Matt Damon would have been a better choice.<br /><br />Kevin Spacey was another big disappointment. His character is so dull, it seems like a bad mix of his character in American Beauty and John Doe in Se7en. It might sound cool, but believe me, it's not.<br /><br />Now, Dylan McDermott's acting is very good. It's about one of the very few good things about this movie. He is just inspired.<br /><br />Morgan Freeman is good but nothing special. He has some really cool lines though.<br /><br />About the story, although it was a bit obvious and exaggerated at times it was good. I was expecting a big twist when Lazerov (Dylan McDermott) was killed, but nothing really happened.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['review'] = df_new['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_data = df_new[[\"review\",\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_x = df_new_data[\"review\"]\n",
    "df_new_y = df_new_data[\"sentiment\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_new.loc[:2500, 'review'].values\n",
    "y_train = df_new.loc[:2500, 'sentiment'].values\n",
    "X_test = df_new.loc[2500:5000, 'review'].values\n",
    "y_test = df_new.loc[2500:5000, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (np.bincount(y_test))\n",
    "#print (np.unique(y_test))\n",
    "\n",
    "#print (np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "param_grid = {'clf__C': [1.0, 10.0, 100.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression())])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('clf',\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\rohit\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'clf__C': [1.0, 10.0, 100.0]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='accuracy',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0} \n",
      "CV Accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8548580567772891"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', tfidf),\n",
    "               ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9540183926429429"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7896841263494602"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = cv.fit_transform(X_train)\n",
    "new_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2501, 28031)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(new_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8208716513394642"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(new_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9684126349460216"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(new_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english')\n",
    "ex = cv.fit_transform([\"Okay maybe it was because I happen to be in Yangchun China when I saw this movie. Maybe it was because I finally had something on TV I could understand or at least read the subtitles, or maybe it was just funny. Whatever it was this movie was worth the time.<br /><br />I had just arrived for my foot and head massage when they gave me the remote so I could watch TV. Usually I would turn the darn thing off but I stumbled upon this crazy movie and got hooked.<br /><br />The plot if you could call it a plot sort of revolves around a cooking competition and sort of is a love story and the food in this movie is the real star. If you like Iron Chef and many of the other cooking shows currently in the reality TV mode, then you will love the scenes with food in this movie.<br /><br />It goes fast and the subtitles are so fast you better be up on speed reading for this one. However the action is mostly slapstick so you don't always have to read the entire subtitle to get the idea.<br /><br />The main actress is lovely eye candy and the main actor isn't bad to look at either. They are both worth watching. Finally if you have some time to kill and want a good laugh this isn't a bad choice for both.<br /><br />I don't speak a word of Chinese but I was totally able to understand the cultural humor of this film. For those who do speak Chinese maybe it is even better. Overall I give this an 8 out of 10 and currently I am even looking to find a copy to have while I stay in China, and keep for when I come back home, it will be a nice reminder for me of my time in Yangchun and a silly afternoon at the massage salon watching a silly movie.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1,  1,  1,  2,  2, 10,  1,  1,  2,  2,  1,  1,\n",
       "         1,  2,  1,  1,  1,  2,  1,  2,  1,  1,  2,  1,  2,  2,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  1,  1,  1,\n",
       "         1,  1,  2,  1,  2,  2,  4,  1,  6,  1,  1,  1,  2,  2,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  2,  2,  1,  1,  1,  1,\n",
       "         1,  1,  2,  1,  3,  1,  1,  3,  2,  1,  1,  1,  2,  1,  2,  2]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " 'able',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'afternoon',\n",
       " 'arrived',\n",
       " 'bad',\n",
       " 'better',\n",
       " 'br',\n",
       " 'candy',\n",
       " 'chef',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'choice',\n",
       " 'come',\n",
       " 'competition',\n",
       " 'cooking',\n",
       " 'copy',\n",
       " 'crazy',\n",
       " 'cultural',\n",
       " 'currently',\n",
       " 'darn',\n",
       " 'don',\n",
       " 'entire',\n",
       " 'eye',\n",
       " 'fast',\n",
       " 'film',\n",
       " 'finally',\n",
       " 'food',\n",
       " 'foot',\n",
       " 'funny',\n",
       " 'gave',\n",
       " 'goes',\n",
       " 'good',\n",
       " 'got',\n",
       " 'happen',\n",
       " 'head',\n",
       " 'home',\n",
       " 'hooked',\n",
       " 'humor',\n",
       " 'idea',\n",
       " 'iron',\n",
       " 'isn',\n",
       " 'just',\n",
       " 'kill',\n",
       " 'laugh',\n",
       " 'like',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'love',\n",
       " 'lovely',\n",
       " 'main',\n",
       " 'massage',\n",
       " 'maybe',\n",
       " 'mode',\n",
       " 'movie',\n",
       " 'nice',\n",
       " 'okay',\n",
       " 'overall',\n",
       " 'plot',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'real',\n",
       " 'reality',\n",
       " 'reminder',\n",
       " 'remote',\n",
       " 'revolves',\n",
       " 'salon',\n",
       " 'saw',\n",
       " 'scenes',\n",
       " 'shows',\n",
       " 'silly',\n",
       " 'slapstick',\n",
       " 'sort',\n",
       " 'speak',\n",
       " 'speed',\n",
       " 'star',\n",
       " 'stay',\n",
       " 'story',\n",
       " 'stumbled',\n",
       " 'subtitle',\n",
       " 'subtitles',\n",
       " 'thing',\n",
       " 'time',\n",
       " 'totally',\n",
       " 'turn',\n",
       " 'tv',\n",
       " 'understand',\n",
       " 'usually',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'word',\n",
       " 'worth',\n",
       " 'yangchun']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_new_x\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "X = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '00000000000',\n",
       " '0000000000001',\n",
       " '00000001',\n",
       " '00001',\n",
       " '00015',\n",
       " '000dm',\n",
       " '000s',\n",
       " '001',\n",
       " '003830',\n",
       " '006',\n",
       " '0069',\n",
       " '007',\n",
       " '0079',\n",
       " '007s',\n",
       " '0080',\n",
       " '0083',\n",
       " '009',\n",
       " '0093638',\n",
       " '00am',\n",
       " '00o',\n",
       " '00pm',\n",
       " '00s',\n",
       " '00schneider',\n",
       " '01',\n",
       " '0126',\n",
       " '0148',\n",
       " '01pm',\n",
       " '02',\n",
       " '020410',\n",
       " '0230',\n",
       " '029',\n",
       " '02i',\n",
       " '02year',\n",
       " '03',\n",
       " '039',\n",
       " '04',\n",
       " '041',\n",
       " '044',\n",
       " '05',\n",
       " '050',\n",
       " '05nomactr',\n",
       " '06',\n",
       " '0615',\n",
       " '06th',\n",
       " '07',\n",
       " '07b',\n",
       " '08',\n",
       " '087',\n",
       " '089',\n",
       " '08th',\n",
       " '09',\n",
       " '0and',\n",
       " '0f',\n",
       " '0ne',\n",
       " '0r',\n",
       " '0s',\n",
       " '0ttmay',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '1000000',\n",
       " '10000000000',\n",
       " '10000000000000',\n",
       " '10000th',\n",
       " '1000lb',\n",
       " '1000s',\n",
       " '1000th',\n",
       " '1001',\n",
       " '1004',\n",
       " '100b',\n",
       " '100bt',\n",
       " '100ft',\n",
       " '100ibs',\n",
       " '100k',\n",
       " '100m',\n",
       " '100miles',\n",
       " '100min',\n",
       " '100mins',\n",
       " '100mph',\n",
       " '100s',\n",
       " '100th',\n",
       " '100times',\n",
       " '100x',\n",
       " '100yards',\n",
       " '101',\n",
       " '101st',\n",
       " '102',\n",
       " '102nd',\n",
       " '103',\n",
       " '104',\n",
       " '1040',\n",
       " '1040a',\n",
       " '1040s',\n",
       " '105',\n",
       " '1050',\n",
       " '105lbs',\n",
       " '106',\n",
       " '106min',\n",
       " '107',\n",
       " '108',\n",
       " '1080',\n",
       " '1080p',\n",
       " '109',\n",
       " '10_',\n",
       " '10a',\n",
       " '10am',\n",
       " '10another',\n",
       " '10as',\n",
       " '10awful',\n",
       " '10bscdb',\n",
       " '10cheesiness',\n",
       " '10clark',\n",
       " '10could',\n",
       " '10cruisweight',\n",
       " '10don',\n",
       " '10ecw',\n",
       " '10eliason',\n",
       " '10em',\n",
       " '10enjoy',\n",
       " '10entertainment',\n",
       " '10fans',\n",
       " '10fifth',\n",
       " '10film',\n",
       " '10fourth',\n",
       " '10franc',\n",
       " '10ft',\n",
       " '10get',\n",
       " '10guinea',\n",
       " '10he',\n",
       " '10hulkamaniacs',\n",
       " '10i',\n",
       " '10if',\n",
       " '10ish',\n",
       " '10it',\n",
       " '10james',\n",
       " '10john',\n",
       " '10k',\n",
       " '10kane',\n",
       " '10lame',\n",
       " '10lines',\n",
       " '10little',\n",
       " '10many',\n",
       " '10may',\n",
       " '10mil',\n",
       " '10min',\n",
       " '10mins',\n",
       " '10minutes',\n",
       " '10molly',\n",
       " '10my',\n",
       " '10not',\n",
       " '10objectionable',\n",
       " '10oh',\n",
       " '10overall',\n",
       " '10p',\n",
       " '10peace',\n",
       " '10pk',\n",
       " '10plot',\n",
       " '10pm',\n",
       " '10ps',\n",
       " '10quality',\n",
       " '10rated',\n",
       " '10rating',\n",
       " '10recommendation',\n",
       " '10recommended',\n",
       " '10replayable',\n",
       " '10s',\n",
       " '10santa',\n",
       " '10second',\n",
       " '10segment',\n",
       " '10seventh',\n",
       " '10sixth',\n",
       " '10star',\n",
       " '10syed',\n",
       " '10th',\n",
       " '10the',\n",
       " '10third',\n",
       " '10this',\n",
       " '10thlevel',\n",
       " '10total',\n",
       " '10umney',\n",
       " '10voluntarily',\n",
       " '10watch',\n",
       " '10william',\n",
       " '10wwe',\n",
       " '10www',\n",
       " '10x',\n",
       " '10yo',\n",
       " '10yr',\n",
       " '10yrs',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11001001',\n",
       " '1100ad',\n",
       " '110min',\n",
       " '110mph',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '1138',\n",
       " '113min',\n",
       " '113minutes',\n",
       " '114',\n",
       " '1146',\n",
       " '115',\n",
       " '116',\n",
       " '116minutes',\n",
       " '117',\n",
       " '11706',\n",
       " '118',\n",
       " '119',\n",
       " '11am',\n",
       " '11f',\n",
       " '11m',\n",
       " '11th',\n",
       " '11ths',\n",
       " '11yr',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '1200f',\n",
       " '1201',\n",
       " '1202',\n",
       " '120kmph',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '12383499143743701',\n",
       " '123k',\n",
       " '124',\n",
       " '1242',\n",
       " '1249',\n",
       " '125',\n",
       " '125m',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '12a',\n",
       " '12hr',\n",
       " '12m',\n",
       " '12mm',\n",
       " '12nd',\n",
       " '12p',\n",
       " '12s',\n",
       " '12th',\n",
       " '12x',\n",
       " '12yr',\n",
       " '12yrs',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '130000',\n",
       " '1300s',\n",
       " '131',\n",
       " '1318',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '135m',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '13848',\n",
       " '139',\n",
       " '13itching',\n",
       " '13james',\n",
       " '13k',\n",
       " '13keats',\n",
       " '13my',\n",
       " '13s',\n",
       " '13th',\n",
       " '14',\n",
       " '140',\n",
       " '1408',\n",
       " '140hp',\n",
       " '141',\n",
       " '1415',\n",
       " '1416',\n",
       " '142',\n",
       " '143',\n",
       " '1433421',\n",
       " '144',\n",
       " '145',\n",
       " '1453',\n",
       " '1454',\n",
       " '145th',\n",
       " '146',\n",
       " '147',\n",
       " '1470',\n",
       " '1473',\n",
       " '1479',\n",
       " '148',\n",
       " '149',\n",
       " '1490s',\n",
       " '1492',\n",
       " '14a',\n",
       " '14afear',\n",
       " '14ai',\n",
       " '14ieme',\n",
       " '14m',\n",
       " '14s',\n",
       " '14th',\n",
       " '14yr',\n",
       " '14ème',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '1500b',\n",
       " '1500s',\n",
       " '150_worst_cases_of_nepotism',\n",
       " '150k',\n",
       " '150m',\n",
       " '150th',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '1532',\n",
       " '1547',\n",
       " '155',\n",
       " '155ºf',\n",
       " '156',\n",
       " '1561',\n",
       " '157',\n",
       " '157th',\n",
       " '158',\n",
       " '1588',\n",
       " '1594',\n",
       " '1596',\n",
       " '1598',\n",
       " '15am',\n",
       " '15apr08',\n",
       " '15ft',\n",
       " '15k',\n",
       " '15mins',\n",
       " '15minutes',\n",
       " '15mts',\n",
       " '15pa',\n",
       " '15pm',\n",
       " '15s',\n",
       " '15structure',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1600s',\n",
       " '1602',\n",
       " '1604',\n",
       " '1606',\n",
       " '160lbs',\n",
       " '161',\n",
       " '1610',\n",
       " '1611',\n",
       " '162',\n",
       " '1620',\n",
       " '1621',\n",
       " '163',\n",
       " '164',\n",
       " '1647',\n",
       " '165',\n",
       " '166',\n",
       " '1660s',\n",
       " '167',\n",
       " '1679',\n",
       " '168',\n",
       " '1683',\n",
       " '169',\n",
       " '1690',\n",
       " '1692',\n",
       " '16a',\n",
       " '16b',\n",
       " '16ieme',\n",
       " '16k',\n",
       " '16mm',\n",
       " '16s',\n",
       " '16th',\n",
       " '16x9',\n",
       " '16ème',\n",
       " '16éme',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '1700s',\n",
       " '1701',\n",
       " '1709',\n",
       " '1709s',\n",
       " '170x',\n",
       " '171',\n",
       " '1710',\n",
       " '1720',\n",
       " '1740',\n",
       " '175',\n",
       " '1754',\n",
       " '1755',\n",
       " '1758',\n",
       " '1759',\n",
       " '1763',\n",
       " '176th',\n",
       " '177',\n",
       " '1775',\n",
       " '1776',\n",
       " '1780s',\n",
       " '1789',\n",
       " '1790',\n",
       " '1790s',\n",
       " '1792',\n",
       " '1793',\n",
       " '1794',\n",
       " '1798',\n",
       " '17it',\n",
       " '17million',\n",
       " '17th',\n",
       " '17yo',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1800hrs',\n",
       " '1800mph',\n",
       " '1800s',\n",
       " '1801',\n",
       " '1805',\n",
       " '1809',\n",
       " '180d',\n",
       " '180s',\n",
       " '181',\n",
       " '1812',\n",
       " '1813',\n",
       " '18137',\n",
       " '1814',\n",
       " '1816',\n",
       " '1817',\n",
       " '182',\n",
       " '1820',\n",
       " '1821',\n",
       " '1823',\n",
       " '1824',\n",
       " '1829',\n",
       " '183',\n",
       " '1830',\n",
       " '1832',\n",
       " '1835',\n",
       " '1836',\n",
       " '1837',\n",
       " '1838',\n",
       " '1839',\n",
       " '1840',\n",
       " '1840s',\n",
       " '1841',\n",
       " '1842',\n",
       " '1844',\n",
       " '1845',\n",
       " '1846',\n",
       " '1847',\n",
       " '185',\n",
       " '1850',\n",
       " '1850ies',\n",
       " '1850s',\n",
       " '1852',\n",
       " '1853',\n",
       " '1854',\n",
       " '1855',\n",
       " '1857',\n",
       " '1858',\n",
       " '1859',\n",
       " '1860',\n",
       " '1860s',\n",
       " '1861',\n",
       " '1862',\n",
       " '1863',\n",
       " '1864',\n",
       " '1865',\n",
       " '1866',\n",
       " '1867',\n",
       " '1868',\n",
       " '1869',\n",
       " '187',\n",
       " '1870',\n",
       " '1870s',\n",
       " '1871',\n",
       " '1872',\n",
       " '1873',\n",
       " '1874',\n",
       " '1875',\n",
       " '1876',\n",
       " '1879',\n",
       " '188',\n",
       " '1880',\n",
       " '1880s',\n",
       " '1881',\n",
       " '1882',\n",
       " '1886',\n",
       " '1887',\n",
       " '1888',\n",
       " '1889',\n",
       " '188o',\n",
       " '1890',\n",
       " '1890s',\n",
       " '1891',\n",
       " '1892',\n",
       " '1893',\n",
       " '1894',\n",
       " '1895',\n",
       " '1896',\n",
       " '1897',\n",
       " '1898',\n",
       " '1899',\n",
       " '18a',\n",
       " '18aeddie',\n",
       " '18ai',\n",
       " '18s',\n",
       " '18th',\n",
       " '18year',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '1900s',\n",
       " '1901',\n",
       " '1902',\n",
       " '1903',\n",
       " '1904',\n",
       " '1905',\n",
       " '1906',\n",
       " '1907',\n",
       " '1908',\n",
       " '1909',\n",
       " '1910',\n",
       " '1910s',\n",
       " '1911',\n",
       " '1912',\n",
       " '1913',\n",
       " '1914',\n",
       " '1915',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '1919',\n",
       " '192',\n",
       " '1920',\n",
       " '1920ies',\n",
       " '1920s',\n",
       " '1921',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1926',\n",
       " '1927',\n",
       " '1928',\n",
       " '1929',\n",
       " '1930',\n",
       " '1930ies',\n",
       " '1930s',\n",
       " '1930sstereotypes',\n",
       " '1931',\n",
       " '1932',\n",
       " '1933',\n",
       " '1934',\n",
       " '1934earl',\n",
       " '1935',\n",
       " '1936',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '193o',\n",
       " '194',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1945when',\n",
       " '1946',\n",
       " '1946oscars',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1949er',\n",
       " '195',\n",
       " '1950',\n",
       " '1950ies',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1952',\n",
       " '1953',\n",
       " '1954',\n",
       " '19546',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '196',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1960sthere',\n",
       " '1961',\n",
       " '1961s',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '197',\n",
       " '1970',\n",
       " '1970ies',\n",
       " '1970ish',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '19784444',\n",
       " '1979',\n",
       " '19796',\n",
       " '197o',\n",
       " '197something',\n",
       " '198',\n",
       " '1980',\n",
       " '1980ies',\n",
       " '1980s',\n",
       " '1980sness',\n",
       " '1981',\n",
       " '1982',\n",
       " '19822',\n",
       " '1982s',\n",
       " '1983',\n",
       " '1983s',\n",
       " '1984',\n",
       " '1984ish',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '198os',\n",
       " '199',\n",
       " '1990',\n",
       " '1990ies',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1991this',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1995s',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '19994',\n",
       " '19k',\n",
       " '19sep2009',\n",
       " '19th',\n",
       " '19thc',\n",
       " '19zkkeonjpo',\n",
       " '1am',\n",
       " '1and',\n",
       " '1as',\n",
       " '1ch',\n",
       " '1d',\n",
       " '1h',\n",
       " '1h30',\n",
       " '1h40',\n",
       " '1h40m',\n",
       " '1h45',\n",
       " '1h53',\n",
       " '1hour',\n",
       " '1hour17min',\n",
       " '1hr',\n",
       " '1it',\n",
       " '1m',\n",
       " '1million',\n",
       " '1min',\n",
       " '1mln',\n",
       " '1o',\n",
       " '1please',\n",
       " '1point',\n",
       " '1ps',\n",
       " '1s',\n",
       " '1sound',\n",
       " '1st',\n",
       " '1ton',\n",
       " '1tv',\n",
       " '1v',\n",
       " '1½',\n",
       " '1ç',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '200000',\n",
       " '20001',\n",
       " '2000ad',\n",
       " '2000anywayz',\n",
       " '2000km',\n",
       " '2000s',\n",
       " '2000sdries',\n",
       " '2001',\n",
       " '2001stars',\n",
       " '2001this',\n",
       " '2002',\n",
       " '2003',\n",
       " '2003i',\n",
       " '2004',\n",
       " '2004s',\n",
       " '2005',\n",
       " '2006',\n",
       " '20061114',\n",
       " '2006parsifal',\n",
       " '2006smackdown',\n",
       " '2007',\n",
       " '2007after',\n",
       " '2008',\n",
       " '20080107',\n",
       " '2008after',\n",
       " '2008very',\n",
       " '2009',\n",
       " '200911290000',\n",
       " '200ft',\n",
       " '200k',\n",
       " '200l',\n",
       " '200mph',\n",
       " '200th',\n",
       " '200x',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2015',\n",
       " '2017',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2021',\n",
       " '2022',\n",
       " '2023',\n",
       " '2024',\n",
       " '2028',\n",
       " '2030',\n",
       " '2031',\n",
       " '2032',\n",
       " '2033',\n",
       " '2035',\n",
       " '2036',\n",
       " '2038',\n",
       " '204',\n",
       " '2040',\n",
       " '2044',\n",
       " '2045',\n",
       " '2046',\n",
       " '2047',\n",
       " '2050',\n",
       " '2053',\n",
       " '2054',\n",
       " '2055',\n",
       " '206',\n",
       " '2060',\n",
       " '2069',\n",
       " '2070',\n",
       " '2080',\n",
       " '2084',\n",
       " '209',\n",
       " '2090',\n",
       " '20c',\n",
       " '20ft',\n",
       " '20g',\n",
       " '20ies',\n",
       " '20k',\n",
       " '20m',\n",
       " '20mins',\n",
       " '20minutes',\n",
       " '20mm',\n",
       " '20mn',\n",
       " '20p',\n",
       " '20perr',\n",
       " '20pm',\n",
       " '20s',\n",
       " '20s1st',\n",
       " '20th',\n",
       " '20ties',\n",
       " '20widow',\n",
       " '20x',\n",
       " '20year',\n",
       " '20yrs',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '2112',\n",
       " '211772166650071408',\n",
       " '2136',\n",
       " '214',\n",
       " '215',\n",
       " '2151',\n",
       " '216',\n",
       " '21699',\n",
       " '21849889for',\n",
       " '21849890',\n",
       " '21849907average',\n",
       " '219',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '2200',\n",
       " '221',\n",
       " '2210',\n",
       " '22101',\n",
       " '222',\n",
       " '223',\n",
       " '225',\n",
       " '2257',\n",
       " '225mins',\n",
       " '227',\n",
       " '227th',\n",
       " '229',\n",
       " '22d',\n",
       " '22h45',\n",
       " '22m',\n",
       " '22mins',\n",
       " '22nd',\n",
       " '23',\n",
       " '230',\n",
       " '230am',\n",
       " '230lbs',\n",
       " '230mph',\n",
       " '231',\n",
       " '232',\n",
       " '233',\n",
       " '234',\n",
       " '2345',\n",
       " '2346',\n",
       " '235th',\n",
       " '236',\n",
       " '237',\n",
       " '2370bce',\n",
       " '238',\n",
       " '23d',\n",
       " '23rd',\n",
       " '24',\n",
       " '240',\n",
       " '2400',\n",
       " '240z',\n",
       " '241',\n",
       " '2412',\n",
       " '2415',\n",
       " '242',\n",
       " '243',\n",
       " '248',\n",
       " '2480',\n",
       " '249',\n",
       " '24h',\n",
       " '24k',\n",
       " '24m30s',\n",
       " '24mar2001',\n",
       " '24p',\n",
       " '24th',\n",
       " '24years',\n",
       " '24yr',\n",
       " '25',\n",
       " '250',\n",
       " '2500',\n",
       " '250000',\n",
       " '2505',\n",
       " '2506',\n",
       " '250you',\n",
       " '2520',\n",
       " '2525',\n",
       " '25c',\n",
       " '25k',\n",
       " '25million',\n",
       " '25min',\n",
       " '25mins',\n",
       " '25s',\n",
       " '25th',\n",
       " '25year',\n",
       " '25yo',\n",
       " '25yrs',\n",
       " '26',\n",
       " '260',\n",
       " '2600',\n",
       " '262',\n",
       " '2642',\n",
       " '265',\n",
       " '269',\n",
       " '26th',\n",
       " '27',\n",
       " '270',\n",
       " '2700',\n",
       " '272',\n",
       " '273',\n",
       " '2737487',\n",
       " '274',\n",
       " '275',\n",
       " '2772',\n",
       " '278',\n",
       " '27th',\n",
       " '27x41',\n",
       " '28',\n",
       " '280',\n",
       " '285',\n",
       " '288',\n",
       " '28s',\n",
       " '28th',\n",
       " '29',\n",
       " '2900',\n",
       " '2900s',\n",
       " '29s',\n",
       " '29th',\n",
       " '2after',\n",
       " '2am',\n",
       " '2c',\n",
       " '2cents',\n",
       " '2cops',\n",
       " '2cvs',\n",
       " '2d',\n",
       " '2dimensional',\n",
       " '2fast',\n",
       " '2ftm',\n",
       " '2furious',\n",
       " '2gether',\n",
       " '2h',\n",
       " '2h30',\n",
       " '2hour',\n",
       " '2hours',\n",
       " '2hr',\n",
       " '2hrs',\n",
       " '2i',\n",
       " '2in',\n",
       " '2inch',\n",
       " '2k',\n",
       " '2lb',\n",
       " '2m',\n",
       " '2min',\n",
       " '2months',\n",
       " '2more',\n",
       " '2nd',\n",
       " '2oo4',\n",
       " '2oo5',\n",
       " '2pac',\n",
       " '2pm',\n",
       " '2point4',\n",
       " '2s',\n",
       " '2warning',\n",
       " '2x',\n",
       " '2x4',\n",
       " '2½',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '3001837218936089620',\n",
       " '300ad',\n",
       " '300c',\n",
       " '300lb',\n",
       " '300lbs',\n",
       " '300mln',\n",
       " '3012',\n",
       " '303',\n",
       " '305',\n",
       " '3064',\n",
       " '30am',\n",
       " '30feet',\n",
       " '30ft',\n",
       " '30i',\n",
       " '30ish',\n",
       " '30k',\n",
       " '30lbs',\n",
       " '30min',\n",
       " '30mins',\n",
       " '30minutes',\n",
       " '30mm',\n",
       " '30mph',\n",
       " '30pm',\n",
       " '30s',\n",
       " '30something',\n",
       " '30th',\n",
       " '30ties',\n",
       " '30yr',\n",
       " '31',\n",
       " '310',\n",
       " '315',\n",
       " '3199',\n",
       " '31st',\n",
       " '32',\n",
       " '320',\n",
       " '3200',\n",
       " '320x180',\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =  train_test_split(X,df_new_y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33500x103771 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2952158 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8566666666666667"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,y_train)\n",
    "nb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model 85.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy of the model\",nb.score(X_test,y_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = [\"Awesome storyline and fantastic acting by every actor. The shown issues are very common in our society which are easily get listened in families and friends.\"]\n",
    "vect = cv.transform(review).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {'negative':0,'positive':1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if nb.predict(vect) == 1:\n",
    "    print(\"positive\")\n",
    "else:\n",
    "        print(\"negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Navie_bayes = open(\"sent_model.pkl\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb,Navie_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Navie_bayes.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = open(\"sent_model.pkl\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pickle.load(sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sent_joblib']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(nb,'sent_joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = joblib.load('sent_joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (33500, 103771)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-40fa80603c40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    759\u001b[0m                         dtype=None)\n\u001b[0;32m    760\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (33500, 103771)"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(new_data,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the reviewmovie is very good\n",
      "bad\n",
      "do u want to try with more reviews \n",
      "0\n",
      " \n",
      " thanks for using our project\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "while (i==1):\n",
    "    rev=input(\"enter the review\")\n",
    "    preprocessor(rev)\n",
    "    sentiment=nb.predict(tfidf.transform([rev]))[0]\n",
    "    if sentiment==1:\n",
    "        print(\"good\")\n",
    "    else :\n",
    "        print(\"bad\")\n",
    "    i=int(input(\"do u want to try with more reviews \\n\")) \n",
    "    if i!=1:\n",
    "        print(\" \\n thanks for using our project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(nb,open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Navie_bayes = open(\"sent_model.pkl\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb,Navie_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Navie_bayes.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = open(\"sent_model.pkl\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pickle.load(sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=pickle.load(open('model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(tfidf,open('tfidf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf=pickle.load(open('tfidf.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(cv,open('cv.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv=pickle.load(open('cv.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(tfidf,\"tfidf_joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_tfidf = joblib.load(\"tfidf_joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(model,\"model_joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(\"model_joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(cv,\"cv_joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_cv = joblib.load(\"cv_joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive_bayesNB = open(\"sent analysis_model.pkl\",\"wb\")\n",
    "#pickle.dump(clf,naive_bayesNB)\n",
    "#naive_bayesNB.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_model =open(\"sent analysis_model.pkl\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = pickle.load(sent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
